{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142057f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Bag of Words:\n",
      "for: 3\n",
      "the: 3\n",
      "free: 2\n",
      "are: 2\n",
      "you: 2\n",
      "tomorrow: 2\n",
      "at: 2\n",
      "office: 2\n",
      "meeting: 2\n",
      "money: 1\n",
      "now: 1\n",
      "hi: 1\n",
      "mom: 1\n",
      "how: 1\n",
      "lowest: 1\n",
      "price: 1\n",
      "your: 1\n",
      "meds: 1\n",
      "we: 1\n",
      "still: 1\n",
      "on: 1\n",
      "dinner: 1\n",
      "win: 1\n",
      "a: 1\n",
      "iphone: 1\n",
      "today: 1\n",
      "let: 1\n",
      "s: 1\n",
      "catch: 1\n",
      "up: 1\n",
      "3: 1\n",
      "pm: 1\n",
      "get: 1\n",
      "50: 1\n",
      "off: 1\n",
      "limited: 1\n",
      "time: 1\n",
      "team: 1\n",
      "in: 1\n",
      "click: 1\n",
      "here: 1\n",
      "prizes: 1\n",
      "can: 1\n",
      "send: 1\n",
      "report: 1\n",
      "\n",
      "SPAM Bag of Words:\n",
      "free: 2\n",
      "for: 2\n",
      "money: 1\n",
      "now: 1\n",
      "lowest: 1\n",
      "price: 1\n",
      "your: 1\n",
      "meds: 1\n",
      "win: 1\n",
      "a: 1\n",
      "iphone: 1\n",
      "today: 1\n",
      "get: 1\n",
      "50: 1\n",
      "off: 1\n",
      "limited: 1\n",
      "time: 1\n",
      "click: 1\n",
      "here: 1\n",
      "prizes: 1\n",
      "\n",
      "HAM Bag of Words:\n",
      "the: 3\n",
      "are: 2\n",
      "you: 2\n",
      "tomorrow: 2\n",
      "at: 2\n",
      "office: 2\n",
      "meeting: 2\n",
      "hi: 1\n",
      "mom: 1\n",
      "how: 1\n",
      "we: 1\n",
      "still: 1\n",
      "on: 1\n",
      "for: 1\n",
      "dinner: 1\n",
      "let: 1\n",
      "s: 1\n",
      "catch: 1\n",
      "up: 1\n",
      "3: 1\n",
      "pm: 1\n",
      "team: 1\n",
      "in: 1\n",
      "can: 1\n",
      "send: 1\n",
      "report: 1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Dataset: (message, class)\n",
    "dataset = [\n",
    "    (\"Free money now!!!\", \"SPAM\"),\n",
    "    (\"Hi mom, how are you?\", \"HAM\"),\n",
    "    (\"Lowest price for your meds\", \"SPAM\"),\n",
    "    (\"Are we still on for dinner?\", \"HAM\"),\n",
    "    (\"Win a free iPhone today\", \"SPAM\"),\n",
    "    (\"Let's catch up tomorrow at the office\", \"HAM\"),\n",
    "    (\"Meeting at 3 PM tomorrow\", \"HAM\"),\n",
    "    (\"Get 50% off, limited time!\", \"SPAM\"),\n",
    "    (\"Team meeting in the office\", \"HAM\"),\n",
    "    (\"Click here for prizes!\", \"SPAM\"),\n",
    "    (\"Can you send the report?\", \"HAM\"),\n",
    "]\n",
    "\n",
    "\n",
    "def clean_and_split(text):\n",
    "    # 1) lowercase\n",
    "    text = text.lower()\n",
    "    # 2) remove punctuation\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    # 3) split into words\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def add_words_to_count(words, count_dict):\n",
    "    for word in words:\n",
    "        if word in count_dict:\n",
    "            count_dict[word] += 1\n",
    "        else:\n",
    "            count_dict[word] = 1\n",
    "\n",
    "\n",
    "def generate_bag_of_words(data):\n",
    "    overall_counts = {}\n",
    "    spam_counts = {}\n",
    "    ham_counts = {}\n",
    "\n",
    "    for message, label in data:\n",
    "        words = clean_and_split(message)\n",
    "\n",
    "        # Count in overall dictionary\n",
    "        add_words_to_count(words, overall_counts)\n",
    "\n",
    "        # Count in class dictionary\n",
    "        if label == \"SPAM\":\n",
    "            add_words_to_count(words, spam_counts)\n",
    "        else:\n",
    "            add_words_to_count(words, ham_counts)\n",
    "\n",
    "    return overall_counts, spam_counts, ham_counts\n",
    "\n",
    "\n",
    "def print_sorted_counts(title, counts):\n",
    "    print(title)\n",
    "    for word, freq in sorted(counts.items(), key=lambda item: item[1], reverse=True):\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "\n",
    "# Run Task 1(a)\n",
    "overall_bow, spam_bow, ham_bow = generate_bag_of_words(dataset)\n",
    "\n",
    "print_sorted_counts(\"Overall Bag of Words:\", overall_bow)\n",
    "print()\n",
    "print_sorted_counts(\"SPAM Bag of Words:\", spam_bow)\n",
    "print()\n",
    "print_sorted_counts(\"HAM Bag of Words:\", ham_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf8a667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 11\n",
      "HAM documents: 6\n",
      "SPAM documents: 5\n",
      "\n",
      "Prior(HAM)  = 6/11 = 0.5455\n",
      "Prior(SPAM) = 5/11 = 0.4545\n"
     ]
    }
   ],
   "source": [
    "# Task 1(b): Calculate class priors\n",
    "\n",
    "# Count how many documents are HAM and SPAM\n",
    "ham_docs = 0\n",
    "spam_docs = 0\n",
    "\n",
    "for message, label in dataset:\n",
    "    if label == \"HAM\":\n",
    "        ham_docs += 1\n",
    "    elif label == \"SPAM\":\n",
    "        spam_docs += 1\n",
    "\n",
    "# Total number of documents\n",
    "total_docs = len(dataset)\n",
    "\n",
    "# Prior probabilities\n",
    "prior_ham = ham_docs / total_docs\n",
    "prior_spam = spam_docs / total_docs\n",
    "\n",
    "print(f\"Total documents: {total_docs}\")\n",
    "print(f\"HAM documents: {ham_docs}\")\n",
    "print(f\"SPAM documents: {spam_docs}\")\n",
    "print()\n",
    "print(f\"Prior(HAM)  = {ham_docs}/{total_docs} = {prior_ham:.4f}\")\n",
    "print(f\"Prior(SPAM) = {spam_docs}/{total_docs} = {prior_spam:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68864f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45\n",
      "Total HAM tokens: 34\n",
      "Total SPAM tokens: 22\n",
      "\n",
      "3: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "50: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "a: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "are: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "at: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "can: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "catch: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "click: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "dinner: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "for: P(token|HAM)=0.0253, P(token|SPAM)=0.0448\n",
      "free: P(token|HAM)=0.0127, P(token|SPAM)=0.0448\n",
      "get: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "here: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "hi: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "how: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "in: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "iphone: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "let: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "limited: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "lowest: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "meds: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "meeting: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "mom: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "money: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "now: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "off: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "office: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "on: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "pm: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "price: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "prizes: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "report: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "s: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "send: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "still: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "team: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "the: P(token|HAM)=0.0506, P(token|SPAM)=0.0149\n",
      "time: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "today: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "tomorrow: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "up: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "we: P(token|HAM)=0.0253, P(token|SPAM)=0.0149\n",
      "win: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n",
      "you: P(token|HAM)=0.0380, P(token|SPAM)=0.0149\n",
      "your: P(token|HAM)=0.0127, P(token|SPAM)=0.0299\n"
     ]
    }
   ],
   "source": [
    "# Task 1(c): Likelihood of each token given HAM or SPAM\n",
    "# Formula with Laplace smoothing:\n",
    "# P(token|class) = (count_in_class + 1) / (total_tokens_in_class + vocabulary_size)\n",
    "\n",
    "vocabulary = sorted(overall_bow.keys())\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "ham_total = sum(ham_bow.values())\n",
    "spam_total = sum(spam_bow.values())\n",
    "\n",
    "likelihood_ham = {}\n",
    "likelihood_spam = {}\n",
    "\n",
    "for token in vocabulary:\n",
    "    likelihood_ham[token] = (ham_bow.get(token, 0) + 1) / (ham_total + vocab_size)\n",
    "    likelihood_spam[token] = (spam_bow.get(token, 0) + 1) / (spam_total + vocab_size)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Total HAM tokens: {ham_total}\")\n",
    "print(f\"Total SPAM tokens: {spam_total}\\n\")\n",
    "\n",
    "for token in vocabulary:\n",
    "    print(f\"{token}: P(token|HAM)={likelihood_ham[token]:.4f}, P(token|SPAM)={likelihood_spam[token]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb18e909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Limited offer, click here!\n",
      "Predicted class: SPAM\n",
      "\n",
      "Sentence: Meeting at 2 PM with the manager.\n",
      "Predicted class: HAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 1(d): Classify test sentences (show class only)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    words = clean_and_split(sentence)\n",
    "\n",
    "    ham_prob = prior_ham\n",
    "    spam_prob = prior_spam\n",
    "\n",
    "    unknown_ham = 1 / (ham_total + vocab_size)\n",
    "    unknown_spam = 1 / (spam_total + vocab_size)\n",
    "\n",
    "    for word in words:\n",
    "        ham_prob *= likelihood_ham.get(word, unknown_ham)\n",
    "        spam_prob *= likelihood_spam.get(word, unknown_spam)\n",
    "\n",
    "    if ham_prob > spam_prob:\n",
    "        return \"HAM\"\n",
    "    return \"SPAM\"\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    predicted = predict_class(sentence)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Predicted class:\", predicted)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1293984f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Limited offer, click here!\n",
      "Predicted class: SPAM\n",
      "\n",
      "Sentence: Meeting at 2 PM with the manager.\n",
      "Predicted class: HAM\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Task 2(a): Using Scikit-Learn (Multinomial Na√Øve Bayes)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Prepare training data\n",
    "train_texts = [text for text, label in dataset]\n",
    "train_labels = [label for text, label in dataset]\n",
    "\n",
    "# Convert text to bag-of-words features\n",
    "vectorizer = CountVectorizer(lowercase=True)\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "\n",
    "# Train model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, train_labels)\n",
    "\n",
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"Limited offer, click here!\",\n",
    "    \"Meeting at 2 PM with the manager.\"\n",
    "]\n",
    "\n",
    "X_test = vectorizer.transform(test_sentences)\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "for sentence, pred in zip(test_sentences, predictions):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Predicted class:\", pred)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
